<!DOCTYPE html>
<html lang="en">
  <head>
    <link rel="stylesheet" href="../nomad_coders/general_style.css" />
    <meta charset="UTF-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Supervised Learning with scikit-learn</title>
  </head>
  <body>
    <header>
      <a href="course_list.html">뒤로 가기</a>
    </header>
    <h1>Supervised Learning with scikit-learn</h1>
    <div id="course0">
      <h2>0. Classification</h2>
      <div id="course0-0" class="idv_class">
        <h3>0-0. Supervised Learning</h3>
        <ul>
          <li>
            What is <b>machine learning</b>?
            <ul>
              <li>
                the art and science of:
                <ul>
                  <li>
                    Giving computers the ability to learn to make decisions from
                    data
                  </li>
                  <li>without being explicitly programmed!</li>
                </ul>
              </li>
              <li>
                Examples:
                <ul>
                  <li>
                    Learning to predict whether an email is spam or not (labeled
                    -> <b>supervised learning</b>)
                  </li>
                  <li>
                    Clustering wikipedia entries into different categories (no
                    label -> <b>unsupervised learning</b>)
                  </li>
                </ul>
              </li>
            </ul>
          </li>
          <li>
            <b>Unsupervised learning</b>
            <ul>
              <li>Uncovering hidden patterns from unlabeled data</li>
              <li>
                Example:
                <ul>
                  <li>
                    Grouping customers into distinct categories (Clustering)
                  </li>
                </ul>
              </li>
            </ul>
          </li>
          <li>
            <b>Reinforcement learning</b>
            <ul>
              <li>
                Software agents interact with an environment
                <ul>
                  <li>Learn how to optimize their behavior</li>
                  <li>Given a system of rewards and punishments</li>
                  <li>Draws inspiration from behavioral psychology</li>
                </ul>
              </li>
              <li>
                Applications
                <ul>
                  <li>Economics</li>
                  <li>Genetics</li>
                  <li>Game playing</li>
                </ul>
              </li>
              <li>
                In 2016, reinforcement learning was used to train
                <b>Google DeepMind's AlphaGo</b> (first computer to defeat the
                world champion in Go)
              </li>
            </ul>
          </li>
          <li>
            <b>Supervised Learning</b>
            <ul>
              <li>Predictor variables/features and a target variable</li>
              <li>
                Aim: Predict the target variable, given the predictor variables
                <ul>
                  <li>
                    <b>Classification</b>: Target variable consists of
                    categories
                  </li>
                  <li><b>Regression</b>: Target variable is continuous</li>
                </ul>
              </li>
            </ul>
          </li>
          <li>
            Naming conventions
            <ul>
              <li>Features = predictor variables = independent variables</li>
              <li>Target variable = dependent variable = response variable</li>
            </ul>
          </li>
          <li>
            Supervised learning
            <ul>
              <li>
                Automate time-consuming or expensive manual tasks
                <ul>
                  <li>ex. doctor's diagnosis</li>
                </ul>
              </li>
              <li>
                Make predictions about the future
                <ul>
                  <li>ex. will a customer click on an ad or not?</li>
                </ul>
              </li>
              <li>
                Need labeled data
                <ul>
                  <li>Historical data with labels</li>
                  <li>Experiments to get labeled data</li>
                  <li>Crowd-sourcing labeled data ex.reCAPTCHA</li>
                </ul>
              </li>
            </ul>
          </li>
          <li>
            scikit-learn
            <ul>
              <li>integrates well with the SciPy stack</li>
            </ul>
          </li>
        </ul>
      </div>
      <div id="course0-1" class="idv_class">
        <h3>0-1. Exploratory data analysis</h3>
        <ul>
          <li>
            The Iris dataset in scikit learn
            <ul>
              <li>
                from sklearn import datasets<br />import pandas as pd<br />import
                numpy as np<br />import matplotlib.pyplot as plt<br />plt.style.use('ggplot')<br />iris
                = datasets.load_iris()<br />type(iris) #->
                sklearn.datasets.base.Bunch
                <ul>
                  <li>
                    cf. Bunch: similar to a dictionary in that it contains
                    key-value pairs
                  </li>
                </ul>
                type(iris.data), type(iris.target) #-> 둘 다 numpy.ndarray<br />iris.data.shape
                #->(150, 4)<br />iris.target_names #-> array(['setosa',
                'versicolor', 'virginica'], dtype='< U10')
              </li>
            </ul>
          </li>
          <li>
            Exploratory Data Analysis(EDA)
            <ul>
              <li>
                X = iris.data<br />y = iris.target<br />df = pd.DataFrame(X,
                columns = iris.feature_names)<br />print(df.head())
              </li>
            </ul>
          </li>
          <li>
            Visual EDA
            <ul>
              <li>
                _ = pd.plotting.scatter_matrix(df, c=y, figsize = [8, 8], s=150,
                market = 'D')
                <ul>
                  <li>c=y #-> data points will be colored by their species</li>
                  <li>s=150 #-> shape</li>
                  <li>marker='D' #-> market size</li>
                </ul>
              </li>
              <li>
                cf. Seaborn's countplot
                <ul>
                  <li>
                    plt.figure() #-> set up a new figure<br />sns.countplot(x='education',
                    hue='party', data=df, palette='RdBu')<br />plt.xticks([0,1],
                    ['No', 'Yes'])<br />plt.show()
                  </li>
                </ul>
              </li>
            </ul>
          </li>
        </ul>
      </div>
      <div id="course0-2" class="idv_class">
        <h3>0-2. The classification challenge</h3>
        <ul>
          <li>
            k-Nearest Neighbors
            <ul>
              <li>
                Basic idea: Predict the label of a data point by
                <ul>
                  <li>Looking at the 'k' closest labeled data points</li>
                  <li>Taking a majority vote</li>
                </ul>
              </li>
              <li></li>
            </ul>
          </li>
          <li>
            Scikit-learn fit and predict
            <ul>
              <li>
                All machine learning models implemented as Python classes
                <ul>
                  <li>
                    They implement the algorithms for learning and predicting
                  </li>
                  <li>Store the information learned from the data</li>
                </ul>
              </li>
              <li>
                Training a model on the data = <b>'fitting'</b> a model to the
                data
                <ul>
                  <li><b>.fit()</b> method</li>
                </ul>
              </li>
              <li>
                To predict the labels of new data: <b>.predict()</b> method
              </li>
              <li>
                from sklearn.neighbors import KNeighborsClassifier<br />knn =
                KNeighborsClassifier(n_neighbors=6)<br />knn.<b>fit</b>(iris['data'],
                iris['target])
              </li>
              <li>
                cf. iris['data'].shape #-> (150, 4)<br />iris['target'].shape
                #-> (150, )
              </li>
            </ul>
          </li>
          <li>
            The scikit-learn API requires that
            <ul>
              <li>the data as a NumPy array or pandas DataFrame</li>
              <li>features take on continous values (not categories)</li>
              <li>no missing values in the data</li>
            </ul>
          </li>
          <li>
            Predicting on unlabeled data
            <ul>
              <li>
                <code>
                  X_new = np.array([5.6, 2.8, 3.9, 1.1], [5.7, 2.6, 3.8, 1.3],
                  [4.7, 3.2, 1.3, 0.2])<br />prediction =
                  knn.<b>predict</b>(X_new)<br />print('Prediction:
                  {}'.format(prediction)) #-> Prediction: [1 1 0]
                </code>
              </li>
            </ul>
          </li>
          <li>
            (꿀팁)X 만들기
            <ul>
              <li><code>X = df.drop("column_name", axis=1)</code></li>
            </ul>
          </li>
        </ul>
      </div>
      <div id="course0-3" class="idv_class">
        <h3>0-3. Measuring model performance</h3>
        <ul>
          <li>
            Measuring model performance
            <ul>
              <li>
                In classification, <b>accuracy</b> is a commonly used metric
              </li>
              <li>Accuracy = Fraction of correct predictions</li>
              <li>Split data into training and test set</li>
              <li>Fit/train the classifier on the training set</li>
              <li>Make predictions on test set</li>
              <li>Compare predictions with the known labels</li>
              <ul>
                <li>
                  <code>
                    from sklearn.model_selection import train_test_split<br /><b
                      >X_train, X_test, y_train, y_test = train_test_split(X, y,
                      test_size=0.3, random_state=21, stratify=y)</b
                    ><br />knn = KNeighborsClassifier(n_neighbors=8)<br />knn.fit(X_train,
                    y_train)<br />y_pred = knn.predict(X_test)
                  </code>
                </li>
                <li>
                  <u>stratify=y</u>: the labels are distributed in train and
                  test sets as they are in the original dataset<br />cf. y is
                  the list or array containing the labels.
                </li>
              </ul>
            </ul>
          </li>
          <li>
            Model complexity
            <ul>
              <li>
                Larger k = smoother decision boundary = less complex
                model(<b>underfitting</b>)
              </li>
              <li>
                Smaller k = more complex model = can lead to <b>overfitting</b>
              </li>
            </ul>
          </li>
          <li>
            the MNIST digit recognition dataset : famous dataset in ML and
            computer vision, and frequently used as a benchmark to evaluate the
            performance of a new model.
            <ul>
              <li>
                <code
                  >from sklearn import datasets<br />digits =
                  datasets.load_digits()</code
                >
              </li>
              <li>
                data points 1797개. 1010번째 데이터 보기
                <ul>
                  <li>
                    <code
                      >plt.imshow(digits.images[1010], cmap = plt.cm.gray_r,
                      interpolation='nearest')<br />plt.show()</code
                    >
                  </li>
                </ul>
              </li>
              <li>
                print the shape of the images and data keys
                <ul>
                  <li>
                    <code>print(digits.images.shape)</code> #-> (1797, 8, 8)<br /><code
                      >print(digits.data.shape)</code
                    >
                    #-> (1797, 64)
                  </li>
                </ul>
              </li>
            </ul>
          </li>
        </ul>
      </div>
    </div>
    <div id="course1">
      <h2>1. Regression</h2>
      <div id="course1-0" class="idv_class">
        <h3>1-0. Introduction to regression</h3>
        <ul>
          <li>
            In regression tasks, the target value is a continuously varying
            variable, such as a country's GDP or the price of a house.
          </li>
          <li>
            Boston housing data
            <ul>
              <li>CRIM: per capita crime rate</li>
              <li>NX: nitric oxcides concentration</li>
              <li>RM: average number of rooms per dwelling</li>
              <li>
                MEDV: the median value of owner occupied homes in thousands of
                dollars.
              </li>
            </ul>
          </li>
          <li>
            Creating feature and target arrays
            <ul>
              <li>
                <code
                  >X = boston.drop('MEDV', axis=1).values<br />y =
                  boston['MEDV'].values</code
                >
              </li>
            </ul>
          </li>
          <li>
            Predicting house value from a single feature
            <ul>
              <li>X_rooms = x[:5]</li>
              <li>
                y = y.reshape(-1, 1)<br />X_rooms = X_rooms.reshape(-1, 1)
              </li>
            </ul>
          </li>
          <li>
            Plotting house value vs. number of rooms
            <ul>
              <li>
                plt.scatter(X_rooms, y)<br />plt.ylabel('Value of house /1000
                ($)')<br />plt.xlabel('Number of rooms')<br />plt.show()
              </li>
            </ul>
          </li>
          <li>
            Fitting a regression model
            <ul>
              <li>
                import numpy as np<br />from sklearn.linear_model import
                LinearRregression<br /><br />reg = LinearRegression()<br />reg.fit(X_rooms,
                y)<br />prediction_space = np.linspace(min(X_rooms),
                max(X_rooms)).reshape(-1, 1)<br /><br />plt.scatter(X_rooms, y,
                color='blue')<br />plt.plot(prediction_space,
                reg.predict(prediction_space), color='black', linewidth=3)<br />plt.show()
              </li>
            </ul>
          </li>
        </ul>
      </div>
      <div id="course1-1" class="idv_class">
        <h3>1-1. The basics of linear regression</h3>
        <ul>
          <li>
            Regression mechanics
            <ul>
              <li>
                y = ax+b
                <ul>
                  <li>y:target</li>
                  <li>x: single feature</li>
                  <li>a, b: parameters of model</li>
                </ul>
              </li>
              <li>How do we choose a and b?</li>
              <li>
                Define an error functions for any given line
                <ul>
                  <li>
                    Choose the line that minimizes the error function(a.k.a. a
                    loss or a cost function)
                  </li>
                </ul>
              </li>
            </ul>
          </li>
          <li>
            The loss function
            <ul>
              <li>
                Ordinary least squares(OLS): Minimize sum of square of residuals
              </li>
            </ul>
          </li>
          <li>
            Linear regression on all features
            <ul>
              <li>
                <code>
                  from sklearn.model_selection import train_test_split<br />from
                  sklearn.linear_model import LinearRegression<br /><br />X_train,
                  X_test, y_train, y_test = train_test_split(X, y,
                  test_size=0.3, random_state=42)<br />reg_all =
                  LinearRegression()<br />reg_all.fit(X_train, y_train)<br />y_pred
                  = reg_all.predict(X_test)</code
                >
              </li>
            </ul>
          </li>
          <li>
            <b>R squared</b>: the default scoring method for linear regression
            <ul>
              <li>
                quantifies the mount of variance in the target variable that is
                predicted from the feature variables.
              </li>
              <li><code>reg_all.score(X_test, y_test)</code></li>
              <li>
                cf. you'll never use linear regression out of the box like this:
                you will most likely wish to use <b><u>regularization</u></b>
                <ul>
                  -> places further constraints on the model coefficients
                </ul>
              </li>
            </ul>
          </li>
          <li>
            <b>the Root Mean Squared Error (RMSE)</b>: another commonly used
            metric to evaluate regression models.
            <ul>
              <li>
                <code
                  >from sklearn.metrics import mean_squared_error<br />...<br />rmse
                  = np.sqrt(mean_squared_error(y_test, y_pred))</code
                >
              </li>
            </ul>
          </li>
        </ul>
      </div>
      <div id="course1-2" class="idv_class">
        <h3>1-2. Cross-validation</h3>
        <ul>
          <li>
            Cross-validation: a vital step in evaluating a model
            <ul>
              <li>
                maximizes the amount of data that is used to train the model.
              </li>
            </ul>
          </li>
          <li>
            Cross-validation motivation
            <ul>
              <li>Model performance is dependent on way the data is split</li>
              <li>Not representative of the model's ability to generalize</li>
              <li>Solution: cross-validation!</li>
            </ul>
          </li>
          <li>
            Cross-validation and model performance
            <ul>
              <li>5 folds = 5-fold CV</li>
              <li>10 folds = 10-fold CV</li>
              <li>k folds = k-fold CV</li>
              <li>
                More folds = More computationally expensive
                <ul>
                  <b>%timeit</b
                  >: to see how long each CV takes<br />-> ex.
                  <code>%timeit cross_val_score(reg, X, y cv=10)</code>
                </ul>
              </li>
            </ul>
          </li>
          <li>
            Cross-validation in scikit-learn
            <ul>
              <li>
                <code
                  >from sklearn.model_selection import cross_val_score<br />from
                  sklearn.linear_model import LinearRegression<br />reg =
                  LinearRegression()<br />cv_results = cross_val_score(reg, X,
                  y, cv=5)<br />print(cv_results)<br />np.mean(cv_results)</code
                >
              </li>
              <li>
                By default, <code>cross_val_score()</code> function uses R
                squared as the metric of choice of regression.
              </li>
            </ul>
          </li>
        </ul>
      </div>
      <div id="course1-3" class="idv_class">
        <h3>1-3. Regularized regression</h3>
        <ul>
          <li>
            Why regularize?
            <ul>
              <li>Recall: Linear regression minimizes a loss function</li>
              <li>It chooses a coefficient for each feature variable</li>
              <li>Large coefficients can lead to overfitting</li>
              <li>Penalizing large coefficients: Regularization</li>
            </ul>
          </li>
          <li>
            Ridge regression
            <ul>
              <li>
                Loss function = OLS loss function +
                <img src="supervised1.png" height="45px" />
              </li>
              <li>Alpha: Parameter we need to choose</li>
              <li>Picking alpha here is similar to picking k in k-NN</li>
              <li>Hyperparameter tuning (More in Chapter 3)</li>
              <li>
                Alpha controls model complexity
                <ul>
                  <li>Alpha = 0 : We get back OLS(Can lead to overfitting)</li>
                  <li>Very high alpha : Can lead to underfitting</li>
                </ul>
              </li>
            </ul>
          </li>
          <li>
            Ridge regression in scikit-learn
            <ul>
              <li>
                <code>
                  from sklearn.linear_model import Ridge<br />X_train, X_test,
                  y_train, y_test = train_test_split(X, y, test_size=0.3,
                  random_state=42)<br />ridge = Ridge(alpha=0.1,
                  normalize=True)<br />ridge.fit(X_train, y_train)<br />ridge_pred
                  = ridge.predict(X_test)<br />ridge.score(X_test, y_test)
                </code>
              </li>
            </ul>
          </li>
          <li>
            Lasso regression
            <ul>
              <li>
                Loss function = OLS loss function +
                <img src="supervised2.png" height="45px" />
              </li>
              <li>
                <code
                  >from sklearn.linear_model import Lasso<br />X_train, X_test,
                  y_train, y_test = train_test_split(X, y, test_size=0.3,
                  random_state=42)<br />lasso = Lasso(alpha=0.1,
                  normalize=True)<br />lasso.fit(X_train, y_train)<br />lasso_pred
                  = lasso.pred(X_test)<br />lasso.score(X_test, y_test)</code
                >
              </li>
            </ul>
          </li>
          <li>
            Lasso regression for feature selection
            <ul>
              <li>Can be used to select important features of a dataset</li>
              <li>
                Shrinks the coefficients of less important features to exactly 0
              </li>
              <li>
                <code
                  >from sklearn.linear_model import Lasso<br />names =
                  boston.drop('MEDV', axis=1).columns<br />lasso =
                  Lasso(alpha=0.1)<br />lasso_coef = lasso.fit(X,y).coef_<br />_
                  = plt.plot(range(len(names)), lasso_coef)<br />_ =
                  plt.xticks(range(len(names)), names, rotation=60)<br />_ =
                  plt.ylabel('Coefficients')<br />plt.show()</code
                >
              </li>
              <img src="supervised3.png" height="250px" />
              <li>(-> the most important predictor is number of rooms)</li>
            </ul>
          </li>
          <li>
            Lasso is great for feature selection, but when building regression
            models, Ridgeg regression should be your first choice.
          </li>
          <li>
            <b>L1 regularization</b> = Lasso's regularization (adding to the
            loss function a penalty term of the <u>absolute value</u> of each
            coefficient multiplied by some alpha)
          </li>
          <li>
            <b>L2 regularizaiton</b> = Ridge regularization (<u
              >squared values</u
            >)
          </li>
          <li></li>
          <li></li>
          <li></li>
        </ul>
      </div>
    </div>
    <div id="course2">
      <h2>2. Fine-tuning your model</h2>
      <div id="course2-0" class="idv_class">
        <h3>2-0. How good is your model?</h3>
      </div>
      <div id="course2-1" class="idv_class">
        <h3>2-1. Logistic regression and the ROC curve</h3>
      </div>
      <div id="course2-2" class="idv_class">
        <h3>2-2. Area under the ROC curve</h3>
      </div>
      <div id="course2-3" class="idv_class">
        <h3>2-3. Hyperparameter tuning</h3>
      </div>
      <div id="course2-4" class="idv_class">
        <h3>2-4. Hold-out set for final evaluation</h3>
      </div>
    </div>
    <div id="course3">
      <h2>3. Preprocessing and pipelines</h2>
      <div id="course3-0" class="idv_class">
        <h3>3-0. Preprocessing data</h3>
      </div>
      <div id="course3-1" class="idv_class">
        <h3>3-1. Handling missing data</h3>
      </div>
      <div id="course3-2" class="idv_class">
        <h3>3-2. Centering and scaling</h3>
      </div>
    </div>
  </body>
</html>
