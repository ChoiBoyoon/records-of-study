<!DOCTYPE html>
<html lang="en">
  <head>
    <link rel="stylesheet" href="../nomad_coders/general_style.css" />
    <meta charset="UTF-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Supervised Learning with scikit-learn</title>
  </head>
  <body>
    <header>
      <a href="course_list.html">뒤로 가기</a>
    </header>
    <h1>Supervised Learning with scikit-learn</h1>
    <div id="course0">
      <h2>0. Classification</h2>
      <div id="course0-0" class="idv_class">
        <h3>0-0. Supervised Learning</h3>
        <ul>
          <li>
            What is <b>machine learning</b>?
            <ul>
              <li>
                the art and science of:
                <ul>
                  <li>
                    Giving computers the ability to learn to make decisions from
                    data
                  </li>
                  <li>without being explicitly programmed!</li>
                </ul>
              </li>
              <li>
                Examples:
                <ul>
                  <li>
                    Learning to predict whether an email is spam or not (labeled
                    -> <b>supervised learning</b>)
                  </li>
                  <li>
                    Clustering wikipedia entries into different categories (no
                    label -> <b>unsupervised learning</b>)
                  </li>
                </ul>
              </li>
            </ul>
          </li>
          <li>
            <b>Unsupervised learning</b>
            <ul>
              <li>Uncovering hidden patterns from unlabeled data</li>
              <li>
                Example:
                <ul>
                  <li>
                    Grouping customers into distinct categories (Clustering)
                  </li>
                </ul>
              </li>
            </ul>
          </li>
          <li>
            <b>Reinforcement learning</b>
            <ul>
              <li>
                Software agents interact with an environment
                <ul>
                  <li>Learn how to optimize their behavior</li>
                  <li>Given a system of rewards and punishments</li>
                  <li>Draws inspiration from behavioral psychology</li>
                </ul>
              </li>
              <li>
                Applications
                <ul>
                  <li>Economics</li>
                  <li>Genetics</li>
                  <li>Game playing</li>
                </ul>
              </li>
              <li>
                In 2016, reinforcement learning was used to train
                <b>Google DeepMind's AlphaGo</b> (first computer to defeat the
                world champion in Go)
              </li>
            </ul>
          </li>
          <li>
            <b>Supervised Learning</b>
            <ul>
              <li>Predictor variables/features and a target variable</li>
              <li>
                Aim: Predict the target variable, given the predictor variables
                <ul>
                  <li>
                    <b>Classification</b>: Target variable consists of
                    categories
                  </li>
                  <li><b>Regression</b>: Target variable is continuous</li>
                </ul>
              </li>
            </ul>
          </li>
          <li>
            Naming conventions
            <ul>
              <li>Features = predictor variables = independent variables</li>
              <li>Target variable = dependent variable = response variable</li>
            </ul>
          </li>
          <li>
            Supervised learning
            <ul>
              <li>
                Automate time-consuming or expensive manual tasks
                <ul>
                  <li>ex. doctor's diagnosis</li>
                </ul>
              </li>
              <li>
                Make predictions about the future
                <ul>
                  <li>ex. will a customer click on an ad or not?</li>
                </ul>
              </li>
              <li>
                Need labeled data
                <ul>
                  <li>Historical data with labels</li>
                  <li>Experiments to get labeled data</li>
                  <li>Crowd-sourcing labeled data ex.reCAPTCHA</li>
                </ul>
              </li>
            </ul>
          </li>
          <li>
            scikit-learn
            <ul>
              <li>integrates well with the SciPy stack</li>
            </ul>
          </li>
        </ul>
      </div>
      <div id="course0-1" class="idv_class">
        <h3>0-1. Exploratory data analysis</h3>
        <ul>
          <li>
            The Iris dataset in scikit learn
            <ul>
              <li>
                from sklearn import datasets<br />import pandas as pd<br />import
                numpy as np<br />import matplotlib.pyplot as plt<br />plt.style.use('ggplot')<br />iris
                = datasets.load_iris()<br />type(iris) #->
                sklearn.datasets.base.Bunch
                <ul>
                  <li>
                    cf. Bunch: similar to a dictionary in that it contains
                    key-value pairs
                  </li>
                </ul>
                type(iris.data), type(iris.target) #-> 둘 다 numpy.ndarray<br />iris.data.shape
                #->(150, 4)<br />iris.target_names #-> array(['setosa',
                'versicolor', 'virginica'], dtype='< U10')
              </li>
            </ul>
          </li>
          <li>
            Exploratory Data Analysis(EDA)
            <ul>
              <li>
                X = iris.data<br />y = iris.target<br />df = pd.DataFrame(X,
                columns = iris.feature_names)<br />print(df.head())
              </li>
            </ul>
          </li>
          <li>
            Visual EDA
            <ul>
              <li>
                _ = pd.plotting.scatter_matrix(df, c=y, figsize = [8, 8], s=150,
                market = 'D')
                <ul>
                  <li>c=y #-> data points will be colored by their species</li>
                  <li>s=150 #-> shape</li>
                  <li>marker='D' #-> market size</li>
                </ul>
              </li>
              <li>
                cf. Seaborn's countplot
                <ul>
                  <li>
                    plt.figure() #-> set up a new figure<br />sns.countplot(x='education',
                    hue='party', data=df, palette='RdBu')<br />plt.xticks([0,1],
                    ['No', 'Yes'])<br />plt.show()
                  </li>
                </ul>
              </li>
            </ul>
          </li>
        </ul>
      </div>
      <div id="course0-2" class="idv_class">
        <h3>0-2. The classification challenge</h3>
        <ul>
          <li>
            k-Nearest Neighbors
            <ul>
              <li>
                Basic idea: Predict the label of a data point by
                <ul>
                  <li>Looking at the 'k' closest labeled data points</li>
                  <li>Taking a majority vote</li>
                </ul>
              </li>
              <li></li>
            </ul>
          </li>
          <li>
            Scikit-learn fit and predict
            <ul>
              <li>
                All machine learning models implemented as Python classes
                <ul>
                  <li>
                    They implement the algorithms for learning and predicting
                  </li>
                  <li>Store the information learned from the data</li>
                </ul>
              </li>
              <li>
                Training a model on the data = <b>'fitting'</b> a model to the
                data
                <ul>
                  <li><b>.fit()</b> method</li>
                </ul>
              </li>
              <li>
                To predict the labels of new data: <b>.predict()</b> method
              </li>
              <li>
                from sklearn.neighbors import KNeighborsClassifier<br />knn =
                KNeighborsClassifier(n_neighbors=6)<br />knn.<b>fit</b>(iris['data'],
                iris['target])
              </li>
              <li>
                cf. iris['data'].shape #-> (150, 4)<br />iris['target'].shape
                #-> (150, )
              </li>
            </ul>
          </li>
          <li>
            The scikit-learn API requires that
            <ul>
              <li>the data as a NumPy array or pandas DataFrame</li>
              <li>features take on continous values (not categories)</li>
              <li>no missing values in the data</li>
            </ul>
          </li>
          <li>
            Predicting on unlabeled data
            <ul>
              <li>
                <code>
                  X_new = np.array([5.6, 2.8, 3.9, 1.1], [5.7, 2.6, 3.8, 1.3],
                  [4.7, 3.2, 1.3, 0.2])<br />prediction =
                  knn.<b>predict</b>(X_new)<br />print('Prediction:
                  {}'.format(prediction)) #-> Prediction: [1 1 0]
                </code>
              </li>
            </ul>
          </li>
          <li>
            (꿀팁)X 만들기
            <ul>
              <li><code>X = df.drop("column_name", axis=1)</code></li>
            </ul>
          </li>
        </ul>
      </div>
      <div id="course0-3" class="idv_class">
        <h3>0-3. Measuring model performance</h3>
        <ul>
          <li>
            Measuring model performance
            <ul>
              <li>
                In classification, <b>accuracy</b> is a commonly used metric
              </li>
              <li>Accuracy = Fraction of correct predictions</li>
              <li>Split data into training and test set</li>
              <li>Fit/train the classifier on the training set</li>
              <li>Make predictions on test set</li>
              <li>Compare predictions with the known labels</li>
              <ul>
                <li>
                  <code>
                    from sklearn.model_selection import train_test_split<br /><b
                      >X_train, X_test, y_train, y_test = train_test_split(X, y,
                      test_size=0.3, random_state=21, stratify=y)</b
                    ><br />knn = KNeighborsClassifier(n_neighbors=8)<br />knn.fit(X_train,
                    y_train)<br />y_pred = knn.predict(X_test)
                  </code>
                </li>
                <li>
                  <u>stratify=y</u>: the labels are distributed in train and
                  test sets as they are in the original dataset<br />cf. y is
                  the list or array containing the labels.
                </li>
              </ul>
            </ul>
          </li>
          <li>
            Model complexity
            <ul>
              <li>
                Larger k = smoother decision boundary = less complex
                model(<b>underfitting</b>)
              </li>
              <li>
                Smaller k = more complex model = can lead to <b>overfitting</b>
              </li>
            </ul>
          </li>
          <li>
            the MNIST digit recognition dataset : famous dataset in ML and
            computer vision, and frequently used as a benchmark to evaluate the
            performance of a new model.
            <ul>
              <li>
                <code
                  >from sklearn import datasets<br />digits =
                  datasets.load_digits()</code
                >
              </li>
              <li>
                data points 1797개. 1010번째 데이터 보기
                <ul>
                  <li>
                    <code
                      >plt.imshow(digits.images[1010], cmap = plt.cm.gray_r,
                      interpolation='nearest')<br />plt.show()</code
                    >
                  </li>
                </ul>
              </li>
              <li>
                print the shape of the images and data keys
                <ul>
                  <li>
                    <code>print(digits.images.shape)</code> #-> (1797, 8, 8)<br /><code
                      >print(digits.data.shape)</code
                    >
                    #-> (1797, 64)
                  </li>
                </ul>
              </li>
            </ul>
          </li>
        </ul>
      </div>
    </div>
    <div id="course1">
      <h2>1. Regression</h2>
      <div id="course1-0" class="idv_class">
        <h3>1-0. Introduction to regression</h3>
        <ul>
          <li>
            In regression tasks, the target value is a continuously varying
            variable, such as a country's GDP or the price of a house.
          </li>
          <li>
            Boston housing data
            <ul>
              <li>CRIM: per capita crime rate</li>
              <li>NX: nitric oxcides concentration</li>
              <li>RM: average number of rooms per dwelling</li>
              <li>
                MEDV: the median value of owner occupied homes in thousands of
                dollars.
              </li>
            </ul>
          </li>
          <li>
            Creating feature and target arrays
            <ul>
              <li>
                <code
                  >X = boston.drop('MEDV', axis=1).values<br />y =
                  boston['MEDV'].values</code
                >
              </li>
            </ul>
          </li>
          <li>
            Predicting house value from a single feature
            <ul>
              <li>X_rooms = x[:5]</li>
              <li>
                y = y.reshape(-1, 1)<br />X_rooms = X_rooms.reshape(-1, 1)
              </li>
            </ul>
          </li>
          <li>
            Plotting house value vs. number of rooms
            <ul>
              <li>
                plt.scatter(X_rooms, y)<br />plt.ylabel('Value of house /1000
                ($)')<br />plt.xlabel('Number of rooms')<br />plt.show()
              </li>
            </ul>
          </li>
          <li>
            Fitting a regression model
            <ul>
              <li>
                import numpy as np<br />from sklearn.linear_model import
                LinearRregression<br /><br />reg = LinearRegression()<br />reg.fit(X_rooms,
                y)<br />prediction_space = np.linspace(min(X_rooms),
                max(X_rooms)).reshape(-1, 1)<br /><br />plt.scatter(X_rooms, y,
                color='blue')<br />plt.plot(prediction_space,
                reg.predict(prediction_space), color='black', linewidth=3)<br />plt.show()
              </li>
            </ul>
          </li>
        </ul>
      </div>
      <div id="course1-1" class="idv_class">
        <h3>1-1. The basics of linear regression</h3>
        <ul>
          <li>
            Regression mechanics
            <ul>
              <li>
                y = ax+b
                <ul>
                  <li>y:target</li>
                  <li>x: single feature</li>
                  <li>a, b: parameters of model</li>
                </ul>
              </li>
              <li>How do we choose a and b?</li>
              <li>
                Define an error functions for any given line
                <ul>
                  <li>
                    Choose the line that minimizes the error function(a.k.a. a
                    loss or a cost function)
                  </li>
                </ul>
              </li>
            </ul>
          </li>
          <li>
            The loss function
            <ul>
              <li>
                Ordinary least squares(OLS): Minimize sum of square of residuals
              </li>
            </ul>
          </li>
          <li>
            Linear regression on all features
            <ul>
              <li>
                <code>
                  from sklearn.model_selection import train_test_split<br />from
                  sklearn.linear_model import LinearRegression<br /><br />X_train,
                  X_test, y_train, y_test = train_test_split(X, y,
                  test_size=0.3, random_state=42)<br />reg_all =
                  LinearRegression()<br />reg_all.fit(X_train, y_train)<br />y_pred
                  = reg_all.predict(X_test)</code
                >
              </li>
            </ul>
          </li>
          <li>
            <b>R squared</b>: the default scoring method for linear regression
            <ul>
              <li>
                quantifies the mount of variance in the target variable that is
                predicted from the feature variables.
              </li>
              <li><code>reg_all.score(X_test, y_test)</code></li>
              <li>
                cf. you'll never use linear regression out of the box like this:
                you will most likely wish to use <b><u>regularization</u></b>
                <ul>
                  -> places further constraints on the model coefficients
                </ul>
              </li>
            </ul>
          </li>
          <li>
            <b>the Root Mean Squared Error (RMSE)</b>: another commonly used
            metric to evaluate regression models.
            <ul>
              <li>
                <code
                  >from sklearn.metrics import mean_squared_error<br />...<br />rmse
                  = np.sqrt(mean_squared_error(y_test, y_pred))</code
                >
              </li>
            </ul>
          </li>
        </ul>
      </div>
      <div id="course1-2" class="idv_class">
        <h3>1-2. Cross-validation</h3>
        <ul>
          <li>
            Cross-validation: a vital step in evaluating a model
            <ul>
              <li>
                maximizes the amount of data that is used to train the model.
              </li>
            </ul>
          </li>
          <li>
            Cross-validation motivation
            <ul>
              <li>Model performance is dependent on way the data is split</li>
              <li>Not representative of the model's ability to generalize</li>
              <li>Solution: cross-validation!</li>
            </ul>
          </li>
          <li>
            Cross-validation and model performance
            <ul>
              <li>5 folds = 5-fold CV</li>
              <li>10 folds = 10-fold CV</li>
              <li>k folds = k-fold CV</li>
              <li>
                More folds = More computationally expensive
                <ul>
                  <b>%timeit</b
                  >: to see how long each CV takes<br />-> ex.
                  <code>%timeit cross_val_score(reg, X, y cv=10)</code>
                </ul>
              </li>
            </ul>
          </li>
          <li>
            Cross-validation in scikit-learn
            <ul>
              <li>
                <code
                  >from sklearn.model_selection import cross_val_score<br />from
                  sklearn.linear_model import LinearRegression<br />reg =
                  LinearRegression()<br />cv_results = cross_val_score(reg, X,
                  y, cv=5)<br />print(cv_results)<br />np.mean(cv_results)</code
                >
              </li>
              <li>
                By default, <code>cross_val_score()</code> function uses R
                squared as the metric of choice of regression.
              </li>
            </ul>
          </li>
        </ul>
      </div>
      <div id="course1-3" class="idv_class">
        <h3>1-3. Regularized regression</h3>
        <ul>
          <li>
            Why regularize?
            <ul>
              <li>Recall: Linear regression minimizes a loss function</li>
              <li>It chooses a coefficient for each feature variable</li>
              <li>Large coefficients can lead to overfitting</li>
              <li>Penalizing large coefficients: Regularization</li>
            </ul>
          </li>
          <li>
            Ridge regression
            <ul>
              <li>
                Loss function = OLS loss function +
                <img src="supervised1.png" height="45px" />
              </li>
              <li>Alpha: Parameter we need to choose</li>
              <li>Picking alpha here is similar to picking k in k-NN</li>
              <li>Hyperparameter tuning (More in Chapter 3)</li>
              <li>
                Alpha controls model complexity
                <ul>
                  <li>Alpha = 0 : We get back OLS(Can lead to overfitting)</li>
                  <li>Very high alpha : Can lead to underfitting</li>
                </ul>
              </li>
            </ul>
          </li>
          <li>
            Ridge regression in scikit-learn
            <ul>
              <li>
                <code>
                  from sklearn.linear_model import Ridge<br />X_train, X_test,
                  y_train, y_test = train_test_split(X, y, test_size=0.3,
                  random_state=42)<br />ridge = Ridge(alpha=0.1,
                  normalize=True)<br />ridge.fit(X_train, y_train)<br />ridge_pred
                  = ridge.predict(X_test)<br />ridge.score(X_test, y_test)
                </code>
              </li>
            </ul>
          </li>
          <li>
            Lasso regression
            <ul>
              <li>
                Loss function = OLS loss function +
                <img src="supervised2.png" height="45px" />
              </li>
              <li>
                <code
                  >from sklearn.linear_model import Lasso<br />X_train, X_test,
                  y_train, y_test = train_test_split(X, y, test_size=0.3,
                  random_state=42)<br />lasso = Lasso(alpha=0.1,
                  normalize=True)<br />lasso.fit(X_train, y_train)<br />lasso_pred
                  = lasso.pred(X_test)<br />lasso.score(X_test, y_test)</code
                >
              </li>
            </ul>
          </li>
          <li>
            Lasso regression for feature selection
            <ul>
              <li>Can be used to select important features of a dataset</li>
              <li>
                Shrinks the coefficients of less important features to exactly 0
              </li>
              <li>
                <code
                  >from sklearn.linear_model import Lasso<br />names =
                  boston.drop('MEDV', axis=1).columns<br />lasso =
                  Lasso(alpha=0.1)<br />lasso_coef = lasso.fit(X,y).coef_<br />_
                  = plt.plot(range(len(names)), lasso_coef)<br />_ =
                  plt.xticks(range(len(names)), names, rotation=60)<br />_ =
                  plt.ylabel('Coefficients')<br />plt.show()</code
                >
              </li>
              <img src="supervised3.png" height="250px" />
              <li>(-> the most important predictor is number of rooms)</li>
            </ul>
          </li>
          <li>
            Lasso is great for feature selection, but when building regression
            models, Ridgeg regression should be your first choice.
          </li>
          <li>
            <b>L1 regularization</b> = Lasso's regularization (adding to the
            loss function a penalty term of the <u>absolute value</u> of each
            coefficient multiplied by some alpha)
          </li>
          <li>
            <b>L2 regularizaiton</b> = Ridge regularization (<u
              >squared values</u
            >)
          </li>
        </ul>
      </div>
    </div>
    <div id="course2">
      <h2>2. Fine-tuning your model</h2>
      <div id="course2-0" class="idv_class">
        <h3>2-0. How good is your model?</h3>
        <ul>
          <li>
            Class imbalance example: Emails
            <ul>
              <li>
                Spam classification
                <ul>
                  <li>99% of emails are real; 1% of emails are spam</li>
                </ul>
              </li>
              <li>
                Could build a classifier that predicts ALL emails as real
                <ul>
                  <li>99% accurate!</li>
                  <li>But horrible at actually classifying spam</li>
                  <li>Fails at its original purpose</li>
                </ul>
              </li>
              <li>
                <b>class imbalance</b>: the situation where one class is more
                frequent
                <ul>
                  <li>Need more nuanced metrics</li>
                </ul>
              </li>
            </ul>
          </li>
          <li>
            Dianosing classification predictions
            <ul>
              <li>
                Confusion matrix<br /><img
                  src="supervised4.png"
                  height="120px"
                />
              </li>
            </ul>
          </li>
          <li>
            Metrics from confusion matrix
            <ul>
              <li>
                <b>precision</b>: tp / (tp + fp)
                <ul>
                  <li>aka ppv(positive predictive value)</li>
                  <li>
                    high precision: not many real emails predicted as spam
                  </li>
                </ul>
              </li>
              <li>
                <b>recall</b>: tp / (tp + fn)
                <ul>
                  <li>
                    also called sensitivity, hit rate, or true positive rate
                  </li>
                  <li>high recall: predicted most spam emails correctly</li>
                </ul>
              </li>
              <li>
                <b>F1 score</b>: 2 * (preciison*recall) / (precision+recall)
                <ul>
                  <li>harmonic mean of precision and recall</li>
                </ul>
              </li>
              <li></li>
            </ul>
          </li>
          <li>
            Confusion matrix in scikit-learn
            <ul>
              <li>
                <code
                  >from sklearn.metrics import classification_report<br />from
                  sklearn.metrics import confusion_matrix<br /><br />knn =
                  KNeighborsClassifier(n_neighbors = 8)<br />X_train, X_test,
                  y_train, y_test = train_test_split(X, y, test_size=0.4,
                  random_state = 42)<br />knn.fit(X_train, y_train)<br />y_pred
                  = knn.predict(X_test)<br /><mark
                    >print(confusion_matrix(y_test, y_pred))<br />print(classification_report(y_test,
                    y_pred))c</mark
                  ></code
                ><br /><br /><img src="supervised5.png" height="140px" />
              </li>
            </ul>
          </li>
        </ul>
      </div>
      <div id="course2-1" class="idv_class">
        <h3>2-1. Logistic regression and the ROC curve</h3>
        <ul>
          <li>
            Logistic regression -> used in classification problems, not
            regression problems
          </li>
          <li>
            Logistic regression for binary classification
            <ul>
              <li>Logistic regression outputs probabilities</li>
              <li>
                If the probability 'p' is greater than 0.5 -> the data is
                labeled '1'
              </li>
              <li>
                If the probability 'p' is less than 0.5 -> the data is labeled
                '0'
              </li>
            </ul>
          </li>
          <li>
            Logistic regression in scikit-learn
            <ul>
              <li>
                <code
                  >from sklearn.linear_model import LogisticRegression<br />from
                  sklearn.model_selection import train_test_split<br /><br />logreg
                  = LogisticRegression()<br />X_train, X_test, y_train, y_test =
                  train_test_split(X, y, test_size = 0.4, random_state = 42)<br />logreg.fit(X_train,
                  y_train)<br />y_pred = logreg.predict(X_test)</code
                >
              </li>
            </ul>
          </li>
          <li>
            Probability thresholds
            <ul>
              <li>
                By default, logistic regression threshold = 0.5 (not specific to
                logistic regression ex. KNN)
              </li>
            </ul>
          </li>
          <li>
            The ROC curve
            <ul>
              <li>the receiver operating characteristic curve</li>
              <li>
                The set of points we get when trying all possible thresholds
              </li>
              <img src="supervised6.png" height="250px" />
            </ul>
          </li>
          <li>
            Plotting the ROC curve
            <ul>
              <li>
                <code
                  >from sklearn.metrics import roc_curve<br />y_pred_prob =
                  logreg.predict_proba(X_test)[:, 1]<br />fpr, tpr, thresholds =
                  roc_curve(y_test, y_pred_prob)<br /><br />plt.plot([0,1],
                  [0,1], 'k--')<br />plt.plot(fpr, tpr, label = 'Logistic
                  Regression')<br />plt.xlabel('False Positive Rate')<br />plt.ylabel('True
                  Positive Rate')<br />plt.title('Logistic Regression ROC
                  curve')<br />plt.show()</code
                >
              </li>
            </ul>
          </li>
        </ul>
      </div>
      <div id="course2-2" class="idv_class">
        <h3>2-2. Area under the ROC curve</h3>
        <ul>
          <li>
            Area under the ROC curve (AUC)
            <ul>
              <li>Larger area under the ROC = better model</li>
              <li>Another popular metric for classification model</li>
              <li>
                If a binary classifier randomly makes guesses(correct 50%)-> AUC
                is 0.5
                <ul>
                  <li>
                    If the AUC is greater more than 0.5, the model is better
                    than random guessing
                  </li>
                </ul>
              </li>
              <img src="supervised7.png" height="260px" />
            </ul>
          </li>
          <li>
            AUC in scikit-learn
            <ul>
              <li>
                <code
                  >from sklearn.metrics import roc_auc_score<br />logreg =
                  LogisticRegressions()<br />X_train, X_test, y_train, y_test =
                  train_test_split(X, y, test_size = 0.4, random_state = 42)<br />logreg.fit(X_train,
                  y_train)<br />y_pred_prob =
                  logreg.predict_proba(X_test)[:,1]<br />roc_auc_score(y_test,
                  y_pred_prob)</code
                >
              </li>
            </ul>
          </li>
          <li>
            AUC using cross-validation
            <ul>
              <li>
                <code
                  >from sklearn.model_selection import cross_val_score<br />cv_scores
                  = cross_val_score(logreg, X, y, cv=5, scoring='roc_auc')<br />print(cv_scores)</code
                >
              </li>
            </ul>
          </li>
        </ul>
      </div>
      <div id="course2-3" class="idv_class">
        <h3>2-3. Hyperparameter tuning</h3>
        <ul>
          <li>
            Hyperparameter tuning
            <ul>
              <li>Linear regression: Choosing parameters</li>
              <li>Ridge/Lasso regression: Choosing alpha</li>
              <li>k-Nearest Neighbors: Choosing n_neighbors</li>
              <li>Parameters like alpha and k: Hyperparameters</li>
              <li>Hyperparameters cannot be learned by fitting the model</li>
            </ul>
          </li>
          <li>
            Choosing the correct hyperparameter
            <ul>
              <li>Try a bunch of different hyperparameter values</li>
              <li>Fit all of them separately</li>
              <li>See how well each performs</li>
              <li>Choose the best performing one</li>
              <li>
                It is essential to use cross-validation (train test split alone
                would risk overfitting the hyperparameter to the test set)
              </li>
              <li>-> Doing so in this fashion is the current standard</li>
            </ul>
          </li>
          <li>
            Grid search cross-validation
            <ul>
              <li>
                can be computationally expensive (-> we can use
                RandomizedSearchCV)
              </li>
              <img src="supervised8.png" height="180px" />
            </ul>
          </li>
          <li>
            GridSerachCV in scikit-learn
            <ul>
              <li>
                <code
                  >from sklearn.model_selection import GridSearchCV<br />param_grid
                  = {'n_neighbors':np.array(1,50)}<br />knn =
                  KNeighborsClassifier()<br />knn_cv = GridSearchCV(knn,
                  param_grid, cv=5)<br />knn_cv.fit(X,y)<br />knn_cv.best_params_
                  #{'n_neighbors': 12}<br />knn_cv.best_score_
                  #0.933216168717</code
                >
              </li>
            </ul>
          </li>
          <li>
            cf. Logistic regression's regularization parameter: <b>C</b>
            <ul>
              <li>C controls the inverse of the regularization strength</li>
              <li>
                A large C can lead to an overfit model, while a small C can lead
                to an underfit model
              </li>
            </ul>
          </li>
          <li>
            RandomizedSearchCV
            <ul>
              <li>not all hyperparameter values are tried out.</li>
              <li>
                a fixed number of hyperparameter settings is sampled from
                specified probabilty distributions.
              </li>
              <li>
                will never outperform GridSearchCV but it's valuable because it
                saves on computation time.
              </li>
            </ul>
          </li>
        </ul>
      </div>
      <div id="course2-4" class="idv_class">
        <h3>2-4. Hold-out set for final evaluation</h3>
        <ul>
          <li>
            Hold-out set reasoning
            <ul>
              <li>How well can the model perform on never before seen data?</li>
              <li>Using ALL data for cross-validation is not ideal</li>
              <li>
                Split data into training and hold-out set at the beginning
              </li>
              <li>Perform grid search cross-validation on training set</li>
              <li>Choose best hyberparameters and evaluate on hold-out set</li>
            </ul>
          </li>

          <li>Lasso: uses the L1 penalty to regularize</li>
          <li>Ridge: uses the L2 penalty to regularize</li>
          <li>
            <b>Elastic net</b>: uses a linear combination of the L1 and L2
            penalties
            <ul>
              <img src="supervised9.png" height="27px" />
              <li>
                <code><b>'l1_ratio'</b></code> in scikit-learn
              </li>
              <li>
                <code
                  >from sklearn.linear_model import ElasticNet<br />l1_space =
                  np.linspace(0, 1, 30)<br />param_grid = {'l1_space',
                  l1_space}<br />elastic_net = ElasticNet()<br />gm_cv =
                  GridSearchCV(elastic_net, param_grid, cv=5)<br />gm_cv.fit(X_train,
                  y_train)<br />y_pred = gm_cv.predict(X_test)<br
                /></code>
              </li>
            </ul>
          </li>
        </ul>
      </div>
    </div>
    <div id="course3">
      <h2>3. Preprocessing and pipelines</h2>
      <div id="course3-0" class="idv_class">
        <h3>3-0. Preprocessing data</h3>
        <ul>
          <li>
            Dealing with categorical features
            <ul>
              <li>
                Scikit-learn will not accept categorical features by default
              </li>
              <li>Need to encode categorical features numerically</li>
              <li>
                Convert to 'dummy variables'
                <ul>
                  <li>0: Observation was NOT that category</li>
                  <li>1: Observation was that category</li>
                </ul>
              </li>
            </ul>
          </li>
          <li>
            Dummy variables
            <ul>
              <img src="supervised10.png" height="110px" />
              <img src="supervised11.png" height="110px" />
            </ul>
          </li>
          <li>
            Dealing with categorical features in Python
            <ul>
              <li>scikit-learn: OneHotEncoder()</li>
              <li>
                pandas: get_dummies()
                <ul>
                  <li>
                    <code
                      >import pandas as pd<br />df = pd.read_csv('auto.csv')<br /><mark
                        >df_origin = pd.get_dummies(df)</mark
                      ><br />df_origin = df_origin.drop('origin_Asia',
                      axis=1)</code
                    >
                  </li>
                  <li>
                    <code
                      >import pandas as pd<br />df = pd.read_csv('auto.csv')<br /><mark
                        >df_origin = pd.get_dummies(df, drop_first=True)</mark
                      ></code
                    >
                  </li>
                </ul>
              </li>
            </ul>
          </li>
          <li>
            Linear regression with dummy variables
            <ul>
              <li>
                <code
                  >from sklearn.model_selection import train_test_split<br />from
                  sklearn.linear_model import Ridge<br />X_train, X_test,
                  y_train, y_test = train_test_split(X, y, test_size=0.3,
                  random_state = 42)<br />ridge = Ridge(alpha=0.5,
                  normalize=True).fit(X_train, y_train)<br />ridge.score(X_test,
                  y_test)</code
                >
              </li>
            </ul>
          </li>
          <li>
            cf. 카테고리별로 boxplot 그리기
            <ul>
              <li>
                <code
                  >import pandas as pd<br />df = pd.read_csv('gapminder.csv')<br />df.boxplot('life',
                  'Region', rot=60) #Region별 life의 boxplot을 보여줌<br />plt.show()</code
                >
              </li>
            </ul>
          </li>
        </ul>
      </div>
      <div id="course3-1" class="idv_class">
        <h3>3-1. Handling missing data</h3>
        <ul>
          <li>
            <b>NaN:</b>
            <ul>
              <li>
                efficient and simplified way of internally representing missing
                data
              </li>
              <li>
                lets us take advantage of pandas methods such as
                <code>.dropna()</code> and <code>.fillna()</code>, as well as
                scikit-learn's Imputation transformer <code>Imputer()</code>
              </li>
            </ul>
          </li>
          <li>
            Dropping missing data
            <ul>
              <li>
                <code
                  >df.insulin.replace(0, np.nan, inplace=True)<br />df.triceps.replace(0,
                  np.nan, inplace=True)<br />df.bmi.replace(0, np.nan,
                  inplace=True)<br />df.info()<br /><br />df = df.dropna() #we
                  lose half of our data!<br />df.shape</code
                >
              </li>
            </ul>
          </li>
          <li>
            Imputing missing data
            <ul>
              <li>Making an educated guess about the missing values</li>
              <li>Example: Using the mean of the non-missing entries</li>
              <li>
                <code
                  >from sklearn.preprocessing import Imputer<br />imp =
                  Imputer(missing_values = 'NaN', strategy = 'mean', axis=0)
                  #axis=0 means we will impute along columns<br />imp.fit(X)<br />X
                  = imp.transform(X) #imputers are known as transformers</code
                >
              </li>
            </ul>
          </li>
          <li>
            Imputing within a pipeline
            <ul>
              <li>
                <code
                  >from sklearn.pipeline import Pipeline<br />from
                  sklearn.preprocessing import Imputer<br />imp =
                  Imputer(missing_values = 'NaN', strategy = 'mean', axis=0)<br />logreg
                  = LogisticRegression()<br />steps = [('imputation', imp),
                  ('logistic_regression', logreg)]<br />pipeline =
                  Pipeline(steps)<br />X_train, X_test, y_train, y_test =
                  train_test_split(X, y, test_size=0.3, random_state=42)<br />pipeline.fit(X_train,
                  y_train)<br />y_pred = pipeline.predict(X_test)<br />pipeline.score(X_test,
                  y_test)</code
                >
              </li>
            </ul>
          </li>
          <li></li>
          <li></li>
        </ul>
      </div>
      <div id="course3-2" class="idv_class">
        <h3>3-2. Centering and scaling</h3>
      </div>
    </div>
  </body>
</html>
